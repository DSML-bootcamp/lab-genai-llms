{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB GenAI - LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI GPT API Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Conversation\n",
    "**Exercise:** Create a simple chatbot that can answer basic questions about a given topic (e.g., history, technology).  \n",
    "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `n`, `stop`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# load and set our key\n",
    "openai_api_key = 'sk-proj-Ca6RLOnhFQTOevVv2tMGT3BlbkFJlwaoLQqxcdb2eDDxaORi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=openai_api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the red panda.\n",
      "\n",
      "the majestic elephant.\n",
      "\n",
      "the playful dolphin.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def gpt_response(inp, message_history, **params):\n",
    "    # We save the user's input\n",
    "    message_history.append({\"role\": \"user\", \"content\": f\"{inp}\"})\n",
    "\n",
    "    # Generate a response from the chatbot model\n",
    "    completion_params = {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"messages\": message_history,\n",
    "        **params  # Include additional parameters\n",
    "    }\n",
    "\n",
    "    chat_completion = client.chat.completions.create(**completion_params\n",
    "        )\n",
    "\n",
    "    # We save the assistant response\n",
    "    message_history.append({\"role\": \"assistant\", \"content\": f\"{chat_completion.choices[0].message.content}\"})\n",
    "\n",
    "    return message_history\n",
    "\n",
    "message_history=[{\"role\": \"system\", \"content\": \"Complete the prompt.\"}]\n",
    "\n",
    "for i in [0,1,2]:\n",
    "    message_history = gpt_response(\"My favourite animal is \",\n",
    "                                   message_history,\n",
    "                                   max_tokens=100,\n",
    "                                   temperature=i)\n",
    "    print(message_history[-1][\"content\"]+\"\\n\") # Print the last response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a dolphin.\n",
      "\n",
      "a red panda.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "message_history=[{\"role\": \"system\", \"content\": \"Complete the prompt.\"}]\n",
    "\n",
    "for i in [0,1]:\n",
    "    message_history = gpt_response(\"My favourite animal is \",\n",
    "                                   message_history,\n",
    "                                   max_tokens=100,\n",
    "                                   top_p=i)\n",
    "    print(message_history[-1][\"content\"]+\"\\n\") # Print the last response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. I can't afford it right now. \n",
      "2. I don't have the money for it. \n",
      "3. I'm a little short on cash. \n",
      "4. I'm on a tight budget. \n",
      "5. I'm low on funds. \n",
      ". \n",
      ". \n",
      ". \n",
      ". \n",
      ". \n",
      ". \n",
      ". \n",
      ". \n",
      ". \n",
      ". \n",
      "...........................\n",
      "\n",
      "1. My wallet is feeling a little light at the moment.\n",
      "2. I'm financially strapped right now.\n",
      "3. Money's tight for me currently.\n",
      "4. Unfortunately, my bank account is running low.\n",
      "5. I've hit a rough patch with finances recently.\n",
      "6.I can't swing that purchase due to financial constraints\n",
      "7.I'm not in a position to make that buy because of money worries \n",
      "8.Im struggling financially so cant afford it sorry \n",
      "9.Sadly, funds are depleted\n",
      "\n"
     ]
    }
   ],
   "source": [
    "message_history=[]\n",
    "\n",
    "for i in [-2,2]:\n",
    "    message_history = gpt_response(\"generate 20 ways to say you can't buy that because you're broke\",\n",
    "                                   message_history,\n",
    "                                   max_tokens=100,\n",
    "                                   presence_penalty=i,\n",
    "                                   frequency_penalty=i)\n",
    "    print(message_history[-1][\"content\"]+\"\\n\")# Print the last response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Temperature (temperature):**\n",
    "\n",
    "- Effect: Controls the randomness of the predictions. Higher values make the model more creative by sampling more diverse outputs. Lower values make it more deterministic.\n",
    "Example: Setting temperature=0.5 might produce more predictable and conservative answers, while temperature=1.0 could generate more varied and surprising responses.\n",
    "\n",
    "**Max Tokens (max_tokens):**\n",
    "\n",
    "- Effect: Limits the maximum number of tokens (words or subwords) in the response. It determines the length of the generated text.\n",
    "Example: If max_tokens=50, the model will generate responses that are up to 50 tokens long.\n",
    "\n",
    "**Top-p (top_p):**\n",
    "\n",
    "- Effect: A probabilistic sampling technique where the model considers only the most likely tokens whose cumulative probability mass is greater than top_p.\n",
    "Example: top_p=0.9 means the model samples from tokens whose cumulative probability adds up to 90% until it chooses the next token.\n",
    "\n",
    "**Frequency Penalty (frequency_penalty) and Presence Penalty (presence_penalty):**\n",
    "\n",
    "- Effect: These penalties adjust the distribution of tokens in the response. Frequency penalty discourages repetitive phrases, while presence penalty encourages a diverse range of words.\n",
    "Example: Increasing frequency_penalty (> 0) reduces repetitive responses, while increasing presence_penalty (> 0) promotes diversity in vocabulary.\n",
    "\n",
    "**Nucleus Sampling (top_p vs top_k):**\n",
    "\n",
    "- Effect: top_p (nucleus sampling) and top_k (top-k sampling) are mutually exclusive and control how the model selects the next word.\n",
    "Example: top_p=0.9 (nucleus sampling) restricts the model to the most likely tokens that make up 90% of the probability mass, whereas top_k=50 (top-k sampling) limits consideration to the top 50 likely tokens.\n",
    "\n",
    "**Stop (stop):**\n",
    "\n",
    "Effect: Controls when the model should stop generating text. It can be either a specific token or a set of tokens that indicate the end of the response.\n",
    "Example: Setting stop=[\".\", \"?\"] tells the model to stop generating once it encounters either a period or a question mark.\n",
    "\n",
    "**How Parameters Affect Responses:**\n",
    "\n",
    "**Temperature:** Higher values lead to more varied and sometimes more imaginative responses, while lower values produce more predictable and safer responses.\n",
    "\n",
    "**Max Tokens:** Affects the length of the response. Larger values allow for more detailed answers, while smaller values constrain the response length.\n",
    "\n",
    "**Top-p and Top-k:** Influence the diversity and specificity of the vocabulary used in responses. Top-p allows for more varied responses based on cumulative probabilities, while top-k limits choices to the most probable tokens.\n",
    "\n",
    "**Frequency and Presence Penalties:** Impact the style and diversity of language in responses. Adjusting these penalties can help control the repetitiveness and creativity of the model's outputs.\n",
    "\n",
    "**Stop Tokens:** Define when the model should end its response, which is useful for controlling the completeness of answers or preventing overly long outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Summarization\n",
    "**Exercise:** Write a script that takes a long text input and summarizes it into a few sentences.  \n",
    "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `best_of`, `logprobs`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Translation\n",
    "**Exercise:** Develop a tool that translates text from one language to another using the API.  \n",
    "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `echo`, `logit_bias`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/Francesco Corda/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f229291f9ad45098805a5b375bb482c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "# Check if you need to specify additional parameters like `name`\n",
    "dataset = load_dataset(huggingface_dataset_name)\n",
    "\n",
    "model_name='google/flan-t5-base'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "def make_prompt(example_indices_full, example_index_to_summarize):\n",
    "    prompt = ''\n",
    "    for index in example_indices_full:\n",
    "        dialogue = dataset['test'][index]['dialogue']\n",
    "        summary = dataset['test'][index]['summary']\n",
    "        \n",
    "        # The stop sequence '{summary}\\n\\n\\n' is important for FLAN-T5. Other models may have their own preferred stop sequence.\n",
    "        prompt += f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "{summary}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    dialogue = dataset['test'][example_index_to_summarize]['dialogue']\n",
    "    \n",
    "    prompt += f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "\"\"\"\n",
    "        \n",
    "    return prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 52\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     45\u001b[0m long_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mThis is a long text that needs to be summarized. It contains multiple paragraphs \u001b[39m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124mwith important information spread throughout. The goal is to condense this text into a \u001b[39m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;124mshorter version that captures the key points. By using summarization techniques, we can \u001b[39m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124mextract the essence of the text and improve readability. This can be particularly useful \u001b[39m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124mfor long articles, research papers, or any content that requires a quick understanding \u001b[39m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124mof the main ideas.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m---> 52\u001b[0m summary \u001b[38;5;241m=\u001b[39m summarize_text(long_text)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(summary)\n",
      "Cell \u001b[1;32mIn[30], line 30\u001b[0m, in \u001b[0;36msummarize_text\u001b[1;34m(text, temperature, max_tokens, top_p, frequency_penalty, presence_penalty, best_of, logprobs)\u001b[0m\n\u001b[0;32m     28\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(best_of):\n\u001b[1;32m---> 30\u001b[0m   response \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mCompletion\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m     31\u001b[0m       engine\u001b[38;5;241m=\u001b[39mengine,\n\u001b[0;32m     32\u001b[0m       prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[0;32m     33\u001b[0m       temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[0;32m     34\u001b[0m       max_tokens\u001b[38;5;241m=\u001b[39mmax_tokens,\n\u001b[0;32m     35\u001b[0m       top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m     36\u001b[0m       frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[0;32m     37\u001b[0m       presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[0;32m     38\u001b[0m       logprobs\u001b[38;5;241m=\u001b[39mlogprobs\n\u001b[0;32m     39\u001b[0m   )\n\u001b[0;32m     41\u001b[0m summary \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m summary\n",
      "File \u001b[1;32mc:\\Users\\Francesco Corda\\Documents\\Aline\\Lib\\site-packages\\openai\\lib\\_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[1;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_args: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol)\n",
      "\u001b[1;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Set your OpenAI API key\n",
    "api_key = 'sk-proj-Ca6RLOnhFQTOevVv2tMGT3BlbkFJlwaoLQqxcdb2eDDxaORi'\n",
    "openai.api_key = api_key\n",
    "\n",
    "\n",
    "def summarize_text(text, temperature=0.7, max_tokens=150, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0, best_of=1, logprobs=None):\n",
    "  \"\"\"\n",
    "  Summarizes a long text input using OpenAI GPT-3 API.\n",
    "\n",
    "  Args:\n",
    "      text: The long text to summarize.\n",
    "      temperature: Controls the randomness of the generated text (higher = more random).\n",
    "      max_tokens: The maximum number of tokens to generate in the summary.\n",
    "      top_p: Influences which words are more likely to be chosen (higher = more common words).\n",
    "      frequency_penalty: Penalty for words that appear often in the input text.\n",
    "      presence_penalty: Penalty for words that appear multiple times within the generated text.\n",
    "      best_of: Number of independent generations to try for getting the best summary.\n",
    "      logprobs: Whether to return the log probabilities of the generated text.\n",
    "\n",
    "  Returns:\n",
    "      A string containing the summarized text.\n",
    "  \"\"\"\n",
    "  prompt = f\"Summarize the following text in a few sentences:\\n{text}\"\n",
    "  engine = \"text-davinci-003\"  # Choose a suitable GPT-3 engine for summarization\n",
    "\n",
    "  response = None\n",
    "  for _ in range(best_of):\n",
    "    response = openai.Completion.create(\n",
    "        engine=engine,\n",
    "        prompt=prompt,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty,\n",
    "        logprobs=logprobs\n",
    "    )\n",
    "  \n",
    "  summary = response.choices[0].text.strip()\n",
    "  return summary\n",
    "\n",
    "# Example usage\n",
    "long_text = \"\"\"This is a long text that needs to be summarized. It contains multiple paragraphs \n",
    "with important information spread throughout. The goal is to condense this text into a \n",
    "shorter version that captures the key points. By using summarization techniques, we can \n",
    "extract the essence of the text and improve readability. This can be particularly useful \n",
    "for long articles, research papers, or any content that requires a quick understanding \n",
    "of the main ideas.\"\"\"\n",
    "\n",
    "summary = summarize_text(long_text)\n",
    "print(summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sentiment Analysis\n",
    "**Exercise:** Implement a sentiment analysis tool that determines the sentiment of a given text (positive, negative, neutral).  \n",
    "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `n`, `logprobs`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a tweet: \n",
      "Positive\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrite a tweet: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m----> 9\u001b[0m     message_history \u001b[38;5;241m=\u001b[39m gpt_response(\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m> \u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     10\u001b[0m                                    message_history,\n\u001b[0;32m     11\u001b[0m                                    temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     12\u001b[0m                                    max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m,\n\u001b[0;32m     13\u001b[0m                                    frequency_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(message_history[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Francesco Corda\\Documents\\Aline\\Lib\\site-packages\\ipykernel\\kernelbase.py:1202\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1200\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[0;32m   1203\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[0;32m   1204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1206\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1207\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Francesco Corda\\Documents\\Aline\\Lib\\site-packages\\ipykernel\\kernelbase.py:1245\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1243\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1244\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1246\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# Lets give an example of sentiment analysis\n",
    "message_history=[{\"role\": \"system\", \"content\": \"Decide whether a Tweet's sentiment is positive, neutral, or negative.\"},\n",
    "             {\"role\": \"user\", \"content\": \"Tweet: \\\"I loved the new Batman movie!\\\"\\nSentiment:\"},\n",
    "              {\"role\": \"assistant\", \"content\": \"Positive\"}\n",
    "  ]\n",
    "print(\"Write a tweet: \")\n",
    "\n",
    "while(True):\n",
    "    message_history = gpt_response(input(\"> \"),\n",
    "                                   message_history,\n",
    "                                   temperature=0,\n",
    "                                   max_tokens=60,\n",
    "                                   frequency_penalty=0.5)\n",
    "    print(message_history[-1][\"content\"])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Completion\n",
    "**Exercise:** Create a text completion application that generates text based on an initial prompt.  \n",
    "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `stop`, `best_of`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To make classic pancakes, you'll need the following ingredients:\n",
      "\n",
      "- 1 cup all-purpose flour\n",
      "- 2 tablespoons sugar\n",
      "- 1 tablespoon baking powder\n",
      "- 1/2 teaspoon salt\n",
      "- 1 cup milk\n",
      "- 1 large egg\n",
      "- 2 tablespoons melted butter or oil\n",
      "- Additional butter or oil for greasing the pan\n",
      "\n",
      "Optional ingredients for flavor variations:\n",
      "- Vanilla extract\n",
      "- Cinnamon\n",
      "- Blueberries, chocolate chips, or diced fruits\n",
      "\n",
      "Remember to mix the wet and dry ingredients separately before combining them to ensure a smooth batter. Cooking pancakes on medium-low heat will help prevent burning and ensure they cook evenly.\n"
     ]
    }
   ],
   "source": [
    "messages=[ # messages parameter must be a list of dictionaries\n",
    "    # can be as short as one message or many back and forth turns.\n",
    "    {\"role\": \"system\", \"content\": \"You are a famous chef. Share your best cooking tips and tricks.\"}, # each dictionary has a role and content\n",
    "    {\"role\": \"user\", \"content\": \"What are the ingredients for making pancakes?\"},\n",
    "  ]\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages = messages,\n",
    "    model = \"gpt-3.5-turbo\",\n",
    ")\n",
    "\n",
    "answer = chat_completion.choices[0].message.content\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS: Google Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Conversation\n",
    "**Exercise:** Create a basic chatbot using Google Vertex AI to answer questions about a given topic.  \n",
    "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `n`, `stop`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Summarization\n",
    "**Exercise:** Develop a script that summarizes long text inputs using Google Vertex AI.  \n",
    "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `best_of`, `logprobs`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Translation\n",
    "**Exercise:** Create a tool that translates text from one language to another using Google Vertex AI.  \n",
    "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `echo`, `logit_bias`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sentiment Analysis\n",
    "**Exercise:** Implement a sentiment analysis tool using Google Vertex AI to determine the sentiment of a given text.  \n",
    "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `n`, `logprobs`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Completion\n",
    "**Exercise:** Develop a text completion application using Google Vertex AI to generate text based on an initial prompt.  \n",
    "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `stop`, `best_of`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
